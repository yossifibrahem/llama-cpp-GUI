{
    "model_path": "C:/Users/yossi/.cache/lm-studio/models/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/Qwen3-4B-Thinking-2507-Q8_0.gguf",
    "alias": "Qwen3-4b",
    "lora_path": "",
    "mmproj_path": "",
    "chat_template": "",
    "reasoning_effort": "",
    "jinja": true,
    "ctx_size": 32768,
    "gpu_layers": 99,
    "threads": "",
    "batch_size": "",
    "cont_batching": true,
    "parallel": "1",
    "flash_attn": true,
    "mlock": false,
    "no_mmap": true,
    "numa": false,
    "moe_cpu_layers": "",
    "draft_model_path": "",
    "draft_gpu_layers": "",
    "draft_tokens": "",
    "host": "127.0.0.1",
    "port": "8080",
    "api_key": "",
    "no_webui": false,
    "embedding": false,
    "verbose": false,
    "custom_args": ""
}